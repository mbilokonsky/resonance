{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resonance Transformers: Testing a Novel Architecture\n",
    "\n",
    "## What Are We Actually Testing?\n",
    "\n",
    "This notebook introduces **Resonance Transformers** - a novel architecture that uses **dual representations** (semantic + structural) with **resonance-biased attention**.\n",
    "\n",
    "### Standard Transformer\n",
    "```\n",
    "Token â†’ Semantic Embedding â†’ Standard Attention â†’ Output\n",
    "```\n",
    "\n",
    "### Resonance Transformer (Novel)\n",
    "```\n",
    "Token â†’ Semantic Embedding â”€â”€â”€â”€â”\n",
    "     â†’ Phase Embedding â”€â”€â”€â”€â”€â”€â”€â”€â”¤â†’ Blended â†’ Resonance Attention â†’ Output\n",
    "       (structural info)        â””â”€ learnable blend Î±\n",
    "```\n",
    "\n",
    "**Key Innovation**: The model learns TWO views of each token:\n",
    "1. **Semantic**: What does this token mean?\n",
    "2. **Structural**: What linguistic structure does it belong to?\n",
    "\n",
    "Resonance attention allows tokens with similar structure to attend to each other more easily.\n",
    "\n",
    "---\n",
    "\n",
    "## The Four Models We're Testing\n",
    "\n",
    "| Model | Architecture | Params | Phase Init | What It Tests |\n",
    "|-------|--------------|--------|------------|---------------|\n",
    "| **Standard** | Vanilla Transformer | 124M | N/A | Traditional baseline |\n",
    "| **Model A** | Resonance | 125M | Random | Does resonance help at all? |\n",
    "| **Model B** | Resonance | 125M | Phonetic | Does phonetic init help resonance? |\n",
    "| **Model C** | Resonance | 53M | Phonetic | Can resonance enable compression? |\n",
    "\n",
    "---\n",
    "\n",
    "## What Would Each Result Mean?\n",
    "\n",
    "### ðŸŽ¯ **ANY Resonance Model Beats Standard** â†’ BREAKTHROUGH!\n",
    "This proves resonance architecture works, regardless of initialization.\n",
    "- Architecture innovation confirmed\n",
    "- Dual representations add value\n",
    "- Could apply to vision, audio, other domains\n",
    "\n",
    "### ðŸŽ¯ **Model B > Model A** â†’ Phonetic init helps!\n",
    "This proves structural initialization matters for phase embeddings.\n",
    "- Opens door to morphological, syntactic, semantic priors\n",
    "- Shows HOW to initialize the structural representation\n",
    "\n",
    "### ðŸŽ¯ **Model C â‰ˆ Model A or Standard** â†’ Compression works!\n",
    "This proves resonance enables parameter efficiency.\n",
    "- Smaller models can punch above their weight\n",
    "- Structure compensates for capacity\n",
    "- Practical for edge deployment\n",
    "\n",
    "### ðŸ“Š **Even \"Failure\" Is Valuable**\n",
    "- If nothing beats standard â†’ Learn what doesn't work\n",
    "- If B â‰ˆ A â†’ Try other structural inits\n",
    "- If C << A â†’ Need more training or better structure\n",
    "\n",
    "---\n",
    "\n",
    "## Important Context: Data Scarcity\n",
    "\n",
    "âš ï¸ **These models are undertrained** (only 102M tokens, need ~2B for optimal):\n",
    "- We're at 4% of Chinchilla-optimal training\n",
    "- Results will be noisy and preliminary\n",
    "- Think of this as a **pilot study** to test feasibility\n",
    "- Follow-up: 15-20 epochs or more data needed for conclusive results\n",
    "\n",
    "**What this means**: If resonance shows ANY advantage in this regime, it's promising!\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria (Reframed)\n",
    "\n",
    "**Strong Success** (any of these):\n",
    "- âœ“ Any resonance model beats Standard by >5%\n",
    "- âœ“ Model B beats Model A by >10%\n",
    "- âœ“ Model C within 10% of Standard\n",
    "\n",
    "**Moderate Success**:\n",
    "- âœ“ Any resonance model within 10% of Standard\n",
    "- âœ“ Model B beats Model A by 5-10%\n",
    "\n",
    "**Learning Outcome** (even if above fails):\n",
    "- Magnitude of any improvements\n",
    "- Whether architecture or initialization matters more\n",
    "- What to try next (more training, different structures)\n",
    "\n",
    "Let's find out! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pronouncing transformers datasets torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import numpy as np\n",
    "import pronouncing\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StandardConfig:\n",
    "    \"\"\"Standard transformer baseline - no resonance mechanisms\"\"\"\n",
    "    name: str = \"standard_baseline\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 256\n",
    "    embed_dim: int = 768\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    ff_dim: int = 3072\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 8\n",
    "    gradient_accumulation: int = 4\n",
    "    learning_rate: float = 3e-4\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResonanceConfig:\n",
    "    \"\"\"Resonance transformer with optional phonetic initialization\"\"\"\n",
    "    name: str = \"base\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 256\n",
    "    embed_dim: int = 768\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    ff_dim: int = 3072\n",
    "    n_frequencies: int = 32\n",
    "    resonance_blend: float = 0.3\n",
    "    resonance_attn_weight: float = 0.1\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 8\n",
    "    gradient_accumulation: int = 4\n",
    "    learning_rate: float = 3e-4\n",
    "    phonetic_init: bool = False  # KEY: enables rhyme-based phase initialization\n",
    "\n",
    "\n",
    "# Model configurations\n",
    "config_random = ResonanceConfig(\n",
    "    name=\"random_phase\",\n",
    "    phonetic_init=False\n",
    ")\n",
    "\n",
    "config_phonetic = ResonanceConfig(\n",
    "    name=\"phonetic_phase\", \n",
    "    phonetic_init=True\n",
    ")\n",
    "\n",
    "# THE CRITICAL TEST: Smaller model with phonetic structure\n",
    "config_small_phonetic = ResonanceConfig(\n",
    "    name=\"small_phonetic\",\n",
    "    embed_dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    ff_dim=2048,\n",
    "    n_frequencies=32,\n",
    "    phonetic_init=True\n",
    ")\n",
    "\n",
    "config_standard = StandardConfig()\n",
    "\n",
    "\n",
    "def count_params(config):\n",
    "    \"\"\"Estimate parameter count\"\"\"\n",
    "    embed = config.vocab_size * config.embed_dim\n",
    "    phase = 0\n",
    "    if hasattr(config, 'n_frequencies'):\n",
    "        phase = config.vocab_size * config.n_frequencies\n",
    "    attn = config.n_layers * (4 * config.embed_dim ** 2)\n",
    "    ff = config.n_layers * (2 * config.embed_dim * config.ff_dim)\n",
    "    return embed + phase + attn + ff\n",
    "\n",
    "\n",
    "print(\"Model Configurations:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Random 126M:    {count_params(config_random)/1e6:.0f}M params, random phase init\")\n",
    "print(f\"  Phonetic 126M:  {count_params(config_phonetic)/1e6:.0f}M params, rhyme-based phase init\")\n",
    "print(f\"  Phonetic 45M:   {count_params(config_small_phonetic)/1e6:.0f}M params, rhyme-based phase init\")\n",
    "print(f\"  Standard:       {count_params(config_standard)/1e6:.0f}M params, no resonance\")\n",
    "print(\"\\nHypothesis: Phonetic 45M matches Random 126M due to phonetic compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic Index Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rhyme_index(tokenizer, vocab_size):\n",
    "    \"\"\"\n",
    "    Build hierarchical rhyme-based index structure.\n",
    "    \n",
    "    Creates rhyme groups where tokens that rhyme together share similar\n",
    "    phase initialization vectors.\n",
    "    \"\"\"\n",
    "    print(\"Building rhyme index...\")\n",
    "    \n",
    "    token_to_rhyme = {}\n",
    "    rhyme_to_tokens = defaultdict(list)\n",
    "    token_to_phones = {}\n",
    "    \n",
    "    for token_id in range(vocab_size):\n",
    "        token_str = tokenizer.decode([token_id]).strip().lower()\n",
    "        token_clean = ''.join(c for c in token_str if c.isalpha())\n",
    "        \n",
    "        if len(token_clean) < 2:\n",
    "            continue\n",
    "            \n",
    "        phones = pronouncing.phones_for_word(token_clean)\n",
    "        \n",
    "        if phones:\n",
    "            rhyme_part = pronouncing.rhyming_part(phones[0])\n",
    "            token_to_rhyme[token_id] = rhyme_part\n",
    "            rhyme_to_tokens[rhyme_part].append(token_id)\n",
    "            token_to_phones[token_id] = phones[0]\n",
    "    \n",
    "    rhyme_index = {\n",
    "        \"token_to_rhyme\": token_to_rhyme,\n",
    "        \"rhyme_to_tokens\": dict(rhyme_to_tokens),\n",
    "        \"token_to_phones\": token_to_phones,\n",
    "        \"n_rhyme_groups\": len(rhyme_to_tokens),\n",
    "        \"tokens_with_rhyme\": len(token_to_rhyme),\n",
    "    }\n",
    "    \n",
    "    print(f\"  Tokens with rhyme: {rhyme_index['tokens_with_rhyme']}/{vocab_size}\")\n",
    "    print(f\"  Unique rhyme groups: {rhyme_index['n_rhyme_groups']}\")\n",
    "    \n",
    "    # Show largest groups\n",
    "    sorted_groups = sorted(rhyme_to_tokens.items(), key=lambda x: -len(x[1]))\n",
    "    print(f\"\\n  Largest rhyme groups:\")\n",
    "    for rhyme, tokens in sorted_groups[:5]:\n",
    "        words = [tokenizer.decode([t]).strip() for t in tokens[:8]]\n",
    "        print(f\"    '{rhyme}' ({len(tokens)} words): {words}\")\n",
    "    \n",
    "    return rhyme_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Transformer (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardAttention(nn.Module):\n",
    "    \"\"\"Vanilla multi-head self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.embed_dim // config.n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(config.embed_dim, 3 * config.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, S, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v).transpose(1, 2).reshape(B, S, -1)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class StandardBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = StandardAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, config.ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_dim, config.embed_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.ln1(x), mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class StandardTransformer(nn.Module):\n",
    "    \"\"\"Vanilla transformer - no resonance, no phase embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        print(f\"\\nBuilding model: {config.name}\")\n",
    "        \n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.pos_embed = nn.Embedding(config.max_seq_len, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            StandardBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len)))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self.n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"  Parameters: {self.n_params/1e6:.2f}M\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        B, S = input_ids.shape\n",
    "        \n",
    "        positions = torch.arange(S, device=input_ids.device)\n",
    "        x = self.token_embed(input_ids) + self.pos_embed(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        mask = self.causal_mask[:S, :S].unsqueeze(0)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        logits = self.lm_head(self.ln_final(x))\n",
    "        \n",
    "        result = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            result[\"loss\"] = F.cross_entropy(\n",
    "                logits[:, :-1, :].contiguous().view(-1, self.config.vocab_size),\n",
    "                labels[:, 1:].contiguous().view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resonance Transformer (Novel Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResonanceEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual embedding: semantic + phase\n",
    "    Phase embeddings can be initialized based on phonetic structure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, rhyme_index=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Semantic embedding (standard)\n",
    "        self.semantic = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        \n",
    "        # Phase embedding (novel)\n",
    "        self.phase = nn.Embedding(config.vocab_size, config.n_frequencies)\n",
    "        self.phase_proj = nn.Linear(config.n_frequencies, config.embed_dim, bias=False)\n",
    "        \n",
    "        # Learnable blend\n",
    "        self.blend = nn.Parameter(torch.full((config.embed_dim,), config.resonance_blend))\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position = nn.Embedding(config.max_seq_len, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Initialize\n",
    "        self._init_weights()\n",
    "        if config.phonetic_init and rhyme_index:\n",
    "            self._init_phonetic_phases(rhyme_index)\n",
    "        else:\n",
    "            self._init_random_phases()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.semantic.weight, std=0.02)\n",
    "        nn.init.normal_(self.phase_proj.weight, std=0.02)\n",
    "        nn.init.normal_(self.position.weight, std=0.02)\n",
    "    \n",
    "    def _init_random_phases(self):\n",
    "        print(\"  Initializing phases: RANDOM\")\n",
    "        nn.init.normal_(self.phase.weight, std=0.3)\n",
    "    \n",
    "    def _init_phonetic_phases(self, rhyme_index):\n",
    "        \"\"\"\n",
    "        Initialize phases so rhyming words are close in phase space.\n",
    "        This is the CORE of the compression hypothesis.\n",
    "        \"\"\"\n",
    "        print(\"  Initializing phases: PHONETIC (rhyme-based)\")\n",
    "        rhyme_to_tokens = rhyme_index[\"rhyme_to_tokens\"]\n",
    "        \n",
    "        # Assign base phase to each rhyme group\n",
    "        rhyme_to_phase = {}\n",
    "        for rhyme in rhyme_to_tokens.keys():\n",
    "            rhyme_to_phase[rhyme] = torch.randn(self.config.n_frequencies) * 0.3\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Initialize all to random\n",
    "            self.phase.weight.data = torch.randn_like(self.phase.weight) * 0.3\n",
    "            \n",
    "            # Override with rhyme structure\n",
    "            phonetic_count = 0\n",
    "            for rhyme, tokens in rhyme_to_tokens.items():\n",
    "                base_phase = rhyme_to_phase[rhyme]\n",
    "                for token_id in tokens:\n",
    "                    # Small noise so they're close but not identical\n",
    "                    self.phase.weight.data[token_id] = base_phase + torch.randn(self.config.n_frequencies) * 0.03\n",
    "                    phonetic_count += 1\n",
    "            \n",
    "            print(f\"    Phonetically initialized: {phonetic_count} tokens\")\n",
    "            print(f\"    Randomly initialized: {self.config.vocab_size - phonetic_count} tokens\")\n",
    "    \n",
    "    def get_resonance_matrix(self, token_ids):\n",
    "        \"\"\"Compute pairwise resonance based on phase similarity\"\"\"\n",
    "        phases = self.phase(token_ids)\n",
    "        phase_diff = phases.unsqueeze(2) - phases.unsqueeze(1)\n",
    "        resonance = torch.cos(phase_diff).mean(dim=-1)\n",
    "        return resonance\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        sem = self.semantic(token_ids)\n",
    "        ph = self.phase(token_ids)\n",
    "        ph_proj = self.phase_proj(ph)\n",
    "        \n",
    "        blend = torch.sigmoid(self.blend)\n",
    "        embeddings = (1 - blend) * sem + blend * ph_proj\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        embeddings = embeddings + self.position(positions)\n",
    "        \n",
    "        resonance = self.get_resonance_matrix(token_ids)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings, resonance\n",
    "\n",
    "\n",
    "class ResonanceAttention(nn.Module):\n",
    "    \"\"\"Attention with resonance bias\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.embed_dim // config.n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(config.embed_dim, 3 * config.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)\n",
    "        self.resonance_weight = nn.Parameter(torch.full((config.n_heads,), config.resonance_attn_weight))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, resonance, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, S, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn + resonance.unsqueeze(1) * self.resonance_weight.view(1, self.n_heads, 1, 1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v).transpose(1, 2).reshape(B, S, -1)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class ResonanceBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = ResonanceAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, config.ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_dim, config.embed_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, resonance, mask=None):\n",
    "        x = x + self.attn(self.ln1(x), resonance, mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResonanceTransformer(nn.Module):\n",
    "    \"\"\"Full resonance transformer with optional phonetic initialization\"\"\"\n",
    "    \n",
    "    def __init__(self, config, rhyme_index=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        print(f\"\\nBuilding model: {config.name}\")\n",
    "        \n",
    "        self.embedding = ResonanceEmbedding(config, rhyme_index)\n",
    "        self.blocks = nn.ModuleList([ResonanceBlock(config) for _ in range(config.n_layers)])\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.semantic.weight\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len)))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self.n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"  Parameters: {self.n_params/1e6:.2f}M\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        B, S = input_ids.shape\n",
    "        \n",
    "        x, resonance = self.embedding(input_ids)\n",
    "        mask = self.causal_mask[:S, :S].unsqueeze(0)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, resonance, mask)\n",
    "        \n",
    "        logits = self.lm_head(self.ln_final(x))\n",
    "        \n",
    "        result = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            result[\"loss\"] = F.cross_entropy(\n",
    "                logits[:, :-1, :].contiguous().view(-1, self.config.vocab_size),\n",
    "                labels[:, 1:].contiguous().view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Tokenized text dataset for causal language modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, documents, tokenizer, max_length=256, max_examples=200000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        print(f\"Tokenizing (max {max_examples} examples)...\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            tokens = tokenizer.encode(doc[\"text\"], truncation=False)\n",
    "            \n",
    "            for j in range(0, len(tokens) - max_length + 1, max_length):\n",
    "                self.examples.append(tokens[j:j + max_length])\n",
    "                \n",
    "                if len(self.examples) >= max_examples:\n",
    "                    break\n",
    "            \n",
    "            if len(self.examples) >= max_examples:\n",
    "                break\n",
    "            \n",
    "            if (i + 1) % 20000 == 0:\n",
    "                print(f\"  {i+1} docs, {len(self.examples)} examples\")\n",
    "        \n",
    "        print(f\"Final: {len(self.examples)} examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "        return {\"input_ids\": tokens, \"labels\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_dataset, val_dataset, n_epochs=20):\n",
    "    \"\"\"Train a model and return results\"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    print(f\"\\nTraining: {config.name}\")\n",
    "    print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "    \n",
    "    results = {\"train_loss\": [], \"val_loss\": [], \"val_ppl\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            loss = model(input_ids, labels=labels)[\"loss\"]\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % config.gradient_accumulation == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"    Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        avg_train = train_loss / len(train_loader)\n",
    "        \n",
    "        # VALIDATE\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                val_loss += model(input_ids, labels=labels)[\"loss\"].item()\n",
    "        \n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        val_ppl = math.exp(avg_val)\n",
    "        \n",
    "        results[\"train_loss\"].append(avg_train)\n",
    "        results[\"val_loss\"].append(avg_val)\n",
    "        results[\"val_ppl\"].append(val_ppl)\n",
    "        \n",
    "        # Log embedding blend if available\n",
    "        blend_str = \"\"\n",
    "        if hasattr(model, 'embedding') and hasattr(model.embedding, 'blend'):\n",
    "            blend = torch.sigmoid(model.embedding.blend).mean().item()\n",
    "            blend_str = f\"\\n    Embedding blend: {blend:.3f}\"\n",
    "        \n",
    "        print(f\"\\n  Epoch {epoch+1}/{n_epochs} ({epoch_time/60:.1f} min)\")\n",
    "        print(f\"    Train Loss: {avg_train:.4f}\")\n",
    "        print(f\"    Val Loss: {avg_val:.4f}\")\n",
    "        print(f\"    Val Perplexity: {val_ppl:.2f}{blend_str}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tokenizer and Build Rhyme Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Build phonetic index\n",
    "rhyme_index = build_rhyme_index(tokenizer, config_random.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading OpenWebText (5%)...\")\n",
    "dataset = load_dataset(\"openwebtext\", split=\"train[:5%]\", trust_remote_code=True)\n",
    "print(f\"Loaded {len(dataset)} documents\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(dataset, tokenizer, max_length=256, max_examples=200000)\n",
    "val_dataset = TextDataset(\n",
    "    dataset.select(range(len(dataset) - 5000, len(dataset))),\n",
    "    tokenizer, \n",
    "    max_length=256, \n",
    "    max_examples=5000\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset)} examples\")\n",
    "print(f\"Val: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment A: Random Phase Init (126M)\n",
    "\n",
    "**What this tests**: Does the resonance architecture help even with random phase initialization?\n",
    "\n",
    "**Architecture**: Resonance Transformer\n",
    "- Dual embeddings (semantic + phase)\n",
    "- Resonance-biased attention\n",
    "- Learnable blend parameter\n",
    "\n",
    "**Phase init**: Random (no phonetic structure)\n",
    "\n",
    "**Why it matters**: \n",
    "- If A beats Standard â†’ resonance architecture itself is valuable\n",
    "- If A â‰ˆ Standard â†’ maybe phonetic init is necessary\n",
    "- Baseline for comparing phonetic vs random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL A: Random Phase Init (126M) - BASELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_a = ResonanceTransformer(config_random, rhyme_index=None).to(device)\n",
    "results_a = train_model(model_a, config_random, train_dataset, val_dataset, n_epochs=2)\n",
    "\n",
    "# Save\n",
    "torch.save(model_a.state_dict(), \"model_a_random_126M.pt\")\n",
    "\n",
    "# Store results\n",
    "all_results = {\"random_126M\": results_a}\n",
    "\n",
    "# Free memory\n",
    "del model_a\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment B: Phonetic Phase Init (126M)\n",
    "\n",
    "**What this tests**: Does phonetic initialization improve the resonance architecture?\n",
    "\n",
    "**Architecture**: Resonance Transformer (same as A)\n",
    "\n",
    "**Phase init**: Phonetic (rhyme-based clustering)\n",
    "- Words that rhyme start with similar phase vectors\n",
    "- ~12,000 tokens initialized based on phonetic structure\n",
    "- Provides structural prior about language\n",
    "\n",
    "**Why it matters**: \n",
    "- **KEY COMPARISON**: B vs A (same architecture, different init)\n",
    "- If B > A â†’ phonetic structure helps the architecture learn better\n",
    "- Magnitude tells us how valuable the initialization is\n",
    "- If B >> A â†’ opens door to other structural priors (morphology, syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL B: Phonetic Phase Init (126M)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_b = ResonanceTransformer(config_phonetic, rhyme_index=rhyme_index).to(device)\n",
    "results_b = train_model(model_b, config_phonetic, train_dataset, val_dataset, n_epochs=2)\n",
    "\n",
    "torch.save(model_b.state_dict(), \"model_b_phonetic_126M.pt\")\n",
    "all_results[\"phonetic_126M\"] = results_b\n",
    "\n",
    "del model_b\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment C: Phonetic Phase Init (53M) - Compression Test\n",
    "\n",
    "**What this tests**: Can resonance + phonetic structure enable parameter compression?\n",
    "\n",
    "**Architecture**: Resonance Transformer (smaller)\n",
    "- 53M params (vs 125M for A/B)\n",
    "- 8 layers instead of 12\n",
    "- 512 embed_dim instead of 768\n",
    "- Same resonance mechanism\n",
    "\n",
    "**Phase init**: Phonetic (same rhyme structure as B)\n",
    "\n",
    "**Why it matters**:\n",
    "- Tests if structure compensates for capacity\n",
    "- If C â‰ˆ A â†’ phonetic structure gives 2.4x parameter efficiency!\n",
    "- If C â‰ˆ Standard â†’ resonance enables practical compression\n",
    "- Even if C < A, gap size tells us partial compression value\n",
    "\n",
    "**Note**: This is the hardest test - smaller models need more data to converge, and we're already undertrained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL C: Phonetic Phase Init (45M) - COMPRESSION TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_c = ResonanceTransformer(config_small_phonetic, rhyme_index=rhyme_index).to(device)\n",
    "results_c = train_model(model_c, config_small_phonetic, train_dataset, val_dataset, n_epochs=2)\n",
    "\n",
    "torch.save(model_c.state_dict(), \"model_c_phonetic_45M.pt\")\n",
    "all_results[\"phonetic_45M\"] = results_c\n",
    "\n",
    "del model_c\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Transformer Baseline (124M)\n",
    "\n",
    "**What this tests**: Traditional transformer performance (the baseline to beat!)\n",
    "\n",
    "**Architecture**: Vanilla Transformer\n",
    "- Single semantic embedding\n",
    "- Standard multi-head attention\n",
    "- No phase embeddings\n",
    "- No resonance mechanism\n",
    "\n",
    "**Why it matters**:\n",
    "- **THE CRITICAL COMPARISON**: Does resonance beat vanilla?\n",
    "- If ANY resonance model (A, B, or C) beats this â†’ breakthrough!\n",
    "- Proves resonance architecture is a genuine improvement\n",
    "- Not just about phonetics, but about the dual-representation mechanism itself\n",
    "\n",
    "**Expected**: Should perform similarly to Model A (both ~125M params, both random init essentially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIONAL: Standard Transformer (126M) - No Resonance\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_std = StandardTransformer(config_standard).to(device)\n",
    "results_std = train_model(model_std, config_standard, train_dataset, val_dataset, n_epochs=2)\n",
    "\n",
    "torch.save(model_std.state_dict(), \"model_standard.pt\")\n",
    "all_results[\"standard_baseline\"] = results_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFinal Perplexity Comparison:\\n\")\n",
    "print(f\"{'Model':<25} {'Params':<12} {'Val PPL':<12} {'vs Baseline'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline_ppl = all_results[\"random_126M\"][\"val_ppl\"][-1]\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    ppl = results[\"val_ppl\"][-1]\n",
    "    \n",
    "    if \"126M\" in name or \"standard\" in name:\n",
    "        params = \"126M\"\n",
    "    elif \"45M\" in name:\n",
    "        params = \"45M\"\n",
    "    else:\n",
    "        params = \"?\"\n",
    "    \n",
    "    if name == \"random_126M\":\n",
    "        vs_baseline = \"(baseline)\"\n",
    "    else:\n",
    "        diff = ((ppl - baseline_ppl) / baseline_ppl) * 100\n",
    "        vs_baseline = f\"{diff:+.1f}%\"\n",
    "    \n",
    "    print(f\"{name:<25} {params:<12} {ppl:<12.2f} {vs_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting The Results\n",
    "\n",
    "### What Each Comparison Tells Us:\n",
    "\n",
    "**1. ANY Resonance vs Standard** â†’ Does the architecture work?\n",
    "- If A, B, or C beats Standard â†’ Resonance transformers are a genuine innovation!\n",
    "- This is the most important result\n",
    "\n",
    "**2. Model B vs Model A** â†’ Does phonetic initialization help?\n",
    "- Both 125M params, same architecture, only diff is phase init\n",
    "- Clean test of whether phonetic structure matters\n",
    "- Magnitude tells us how valuable the initialization is\n",
    "\n",
    "**3. Model C vs Model A** â†’ Does structure enable compression?\n",
    "- 53M vs 125M params (2.4x difference)\n",
    "- If C â‰ˆ A â†’ phonetic structure compensates for 70M fewer params!\n",
    "- Even partial compression (C closer to A than expected) is valuable\n",
    "\n",
    "**4. Model B vs Standard** â†’ Best of both worlds?\n",
    "- Resonance architecture + phonetic initialization\n",
    "- Should be the best performing model if both innovations help\n",
    "\n",
    "### Remember: We're Undertrained!\n",
    "\n",
    "All models have only 4% of optimal training data. This means:\n",
    "- Expect high perplexities (80-120, not 20-30)\n",
    "- ANY advantage is amplified in importance\n",
    "- Follow-up with 15-20 epochs will be more conclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPOTHESIS TEST: DOES PHONETIC STRUCTURE COMPRESS?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "phonetic_45_ppl = all_results[\"phonetic_45M\"][\"val_ppl\"][-1]\n",
    "random_126_ppl = all_results[\"random_126M\"][\"val_ppl\"][-1]\n",
    "\n",
    "if phonetic_45_ppl <= random_126_ppl * 1.1:  # Within 10%\n",
    "    print(f\"\"\"\n",
    "âœ“ SUCCESS: 45M phonetic model achieves {phonetic_45_ppl:.2f} perplexity\n",
    "  vs 126M random model at {random_126_ppl:.2f} perplexity.\n",
    "  \n",
    "  This represents ~2.8x parameter efficiency from phonetic structure.\n",
    "  \n",
    "  YOUR THEORY IS SUPPORTED: Rhyme-based indexing compresses better.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "âœ— 45M phonetic model ({phonetic_45_ppl:.2f}) did not match 126M baseline ({random_126_ppl:.2f}).\n",
    "  \n",
    "  Gap: {((phonetic_45_ppl - random_126_ppl) / random_126_ppl * 100):.1f}%\n",
    "  \n",
    "  Possible reasons:\n",
    "  - Need more training epochs\n",
    "  - Need more/better data\n",
    "  - Phonetic structure helps but doesn't fully compensate for size\n",
    "\"\"\")\n",
    "\n",
    "# Secondary test\n",
    "if \"phonetic_126M\" in all_results:\n",
    "    phonetic_126_ppl = all_results[\"phonetic_126M\"][\"val_ppl\"][-1]\n",
    "    \n",
    "    if phonetic_126_ppl < random_126_ppl:\n",
    "        improvement = ((random_126_ppl - phonetic_126_ppl) / random_126_ppl) * 100\n",
    "        print(f\"\"\"\n",
    "âœ“ PARTIAL SUCCESS: At same size (126M), phonetic init beats random:\n",
    "  Phonetic: {phonetic_126_ppl:.2f}\n",
    "  Random: {random_126_ppl:.2f}\n",
    "  Improvement: {improvement:.1f}%\n",
    "  \n",
    "  Phonetic structure provides measurable benefit.\n",
    "\"\"\")\n",
    "    else:\n",
    "        print(f\"\"\"\n",
    "Note: Phonetic 126M ({phonetic_126_ppl:.2f}) did not beat random 126M ({random_126_ppl:.2f}).\n",
    "The phonetic initialization may not be providing the expected inductive bias.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Did We Learn? Next Steps\n",
    "\n",
    "### Immediate Analysis\n",
    "\n",
    "After this run completes, examine:\n",
    "\n",
    "**1. Architecture Validation**\n",
    "- Did ANY resonance model beat Standard?\n",
    "- If yes â†’ You've validated a novel architecture! ðŸŽ‰\n",
    "- If no â†’ Need more training or design refinement\n",
    "\n",
    "**2. Initialization Value**\n",
    "- How much better is B than A? (phonetic vs random)\n",
    "- 5% improvement â†’ modest but real\n",
    "- 10%+ improvement â†’ very significant\n",
    "\n",
    "**3. Compression Potential**\n",
    "- How close is C to A or Standard?\n",
    "- Within 10% â†’ strong compression evidence\n",
    "- Within 20% â†’ partial compression\n",
    "\n",
    "---\n",
    "\n",
    "### Follow-Up Experiments\n",
    "\n",
    "**If Resonance Shows Promise:**\n",
    "\n",
    "1. **More Training** - Run 15-20 epochs to escape undertrained regime\n",
    "2. **Ablation Studies** - Test phonetic init without resonance (and vice versa)\n",
    "3. **Other Structures** - Try morphological, syntactic, semantic priors\n",
    "4. **Visualization** - t-SNE of phase space, attention heatmaps\n",
    "5. **Scaling** - Larger models, different domains\n",
    "\n",
    "---\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "**You're exploring dual-representation learning with structure-biased attention.**\n",
    "\n",
    "This is a research program, not a single experiment. Whatever the results, you've learned something valuable! ðŸ˜ðŸ”¬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
