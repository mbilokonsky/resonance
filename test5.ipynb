{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e5f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/anaconda3/envs/A_10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"pronouncing\", \"-q\"])\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "import numpy as np\n",
    "import pronouncing\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82f8ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to compare:\n",
      "  A) Random 126M:    125M params, random phase init\n",
      "  B) Phonetic 126M:  125M params, rhyme-based phase init\n",
      "  C) Phonetic 45M:   53M params, rhyme-based phase init\n",
      "\n",
      "Hypothesis: C matches or beats A despite having 1/3 the parameters\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ResonanceConfig:\n",
    "    name: str = \"base\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 256\n",
    "    embed_dim: int = 768\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    ff_dim: int = 3072\n",
    "    n_frequencies: int = 32\n",
    "    resonance_blend: float = 0.3\n",
    "    resonance_attn_weight: float = 0.1\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 8\n",
    "    gradient_accumulation: int = 4\n",
    "    learning_rate: float = 3e-4\n",
    "    \n",
    "    # The key difference\n",
    "    phonetic_init: bool = False\n",
    "\n",
    "# Config A: Random phase init (baseline)\n",
    "config_random = ResonanceConfig(\n",
    "    name=\"random_phase\",\n",
    "    phonetic_init=False\n",
    ")\n",
    "\n",
    "# Config B: Phonetic phase init (your theory)\n",
    "config_phonetic = ResonanceConfig(\n",
    "    name=\"phonetic_phase\", \n",
    "    phonetic_init=True\n",
    ")\n",
    "\n",
    "# Config C: SMALLER model with phonetic init (the real test)\n",
    "config_small_phonetic = ResonanceConfig(\n",
    "    name=\"small_phonetic\",\n",
    "    embed_dim=512,       # Smaller\n",
    "    n_layers=8,          # Fewer layers\n",
    "    n_heads=8,\n",
    "    ff_dim=2048,\n",
    "    n_frequencies=32,\n",
    "    phonetic_init=True\n",
    ")\n",
    "\n",
    "def count_params(config):\n",
    "    embed = config.vocab_size * config.embed_dim\n",
    "    phase = config.vocab_size * config.n_frequencies\n",
    "    attn = config.n_layers * (4 * config.embed_dim ** 2)\n",
    "    ff = config.n_layers * (2 * config.embed_dim * config.ff_dim)\n",
    "    return embed + phase + attn + ff\n",
    "\n",
    "print(\"Models to compare:\")\n",
    "print(f\"  A) Random 126M:    {count_params(config_random)/1e6:.0f}M params, random phase init\")\n",
    "print(f\"  B) Phonetic 126M:  {count_params(config_phonetic)/1e6:.0f}M params, rhyme-based phase init\")\n",
    "print(f\"  C) Phonetic 45M:   {count_params(config_small_phonetic)/1e6:.0f}M params, rhyme-based phase init\")\n",
    "print()\n",
    "print(\"Hypothesis: C matches or beats A despite having 1/3 the parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45014df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building rhyme index...\n",
      "  Tokens with rhyme: 35431/50257\n",
      "  Unique rhyme groups: 8045\n",
      "\n",
      "  Largest rhyme groups:\n",
      "    'EY1 SH AH0 N' (422 words): ['formation', 'information', 'situation', 'population', 'nation', 'application', 'investigation', 'administration']\n",
      "    'IY1' (405 words): ['he', 've', 'be', 'ac', 'he', 'ce', 'we', 'de']\n",
      "    'UW1' (289 words): ['ou', 'to', 'you', 'qu', 'su', 'do', 'who', 'ru']\n",
      "    'EY1' (281 words): ['re', 're', 'ay', 'se', 'se', 'ok', 'they', 'play']\n",
      "    'OW1' (257 words): ['ro', 'ow', 'pro', 'go', 'so', 'au', 'so', 'no']\n"
     ]
    }
   ],
   "source": [
    "def build_rhyme_index(tokenizer, vocab_size):\n",
    "    \"\"\"\n",
    "    Build the rhyme-based index structure.\n",
    "    This is the core of your compression theory.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Building rhyme index...\")\n",
    "    \n",
    "    # Map each token to its rhyme signature\n",
    "    token_to_rhyme = {}\n",
    "    rhyme_to_tokens = defaultdict(list)\n",
    "    \n",
    "    # Also track phonetic features for richer structure\n",
    "    token_to_phones = {}\n",
    "    \n",
    "    for token_id in range(vocab_size):\n",
    "        token_str = tokenizer.decode([token_id]).strip().lower()\n",
    "        token_clean = ''.join(c for c in token_str if c.isalpha())\n",
    "        \n",
    "        if len(token_clean) < 2:\n",
    "            continue\n",
    "        \n",
    "        phones = pronouncing.phones_for_word(token_clean)\n",
    "        \n",
    "        if phones:\n",
    "            rhyme_part = pronouncing.rhyming_part(phones[0])\n",
    "            token_to_rhyme[token_id] = rhyme_part\n",
    "            rhyme_to_tokens[rhyme_part].append(token_id)\n",
    "            token_to_phones[token_id] = phones[0]\n",
    "    \n",
    "    # Create hierarchical structure\n",
    "    # Level 1: Rhyme groups (words that rhyme together)\n",
    "    # Level 2: Phonetic similarity within groups\n",
    "    \n",
    "    rhyme_index = {\n",
    "        \"token_to_rhyme\": token_to_rhyme,\n",
    "        \"rhyme_to_tokens\": dict(rhyme_to_tokens),\n",
    "        \"token_to_phones\": token_to_phones,\n",
    "        \"n_rhyme_groups\": len(rhyme_to_tokens),\n",
    "        \"tokens_with_rhyme\": len(token_to_rhyme),\n",
    "    }\n",
    "    \n",
    "    # Stats\n",
    "    print(f\"  Tokens with rhyme: {rhyme_index['tokens_with_rhyme']}/{vocab_size}\")\n",
    "    print(f\"  Unique rhyme groups: {rhyme_index['n_rhyme_groups']}\")\n",
    "    \n",
    "    # Largest groups\n",
    "    sorted_groups = sorted(rhyme_to_tokens.items(), key=lambda x: -len(x[1]))\n",
    "    print(f\"\\n  Largest rhyme groups:\")\n",
    "    for rhyme, tokens in sorted_groups[:5]:\n",
    "        words = [tokenizer.decode([t]).strip() for t in tokens[:8]]\n",
    "        print(f\"    '{rhyme}' ({len(tokens)} words): {words}\")\n",
    "    \n",
    "    return rhyme_index\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "rhyme_index = build_rhyme_index(tokenizer, config_random.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a6b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResonanceEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer with optional phonetic phase initialization.\n",
    "    \n",
    "    The key insight: if phonetic_init=True, words that rhyme\n",
    "    start close in phase-space. This gives the model a structural\n",
    "    prior that should require fewer parameters to exploit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, rhyme_index=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Semantic embedding\n",
    "        self.semantic = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        \n",
    "        # Phase embedding\n",
    "        self.phase = nn.Embedding(config.vocab_size, config.n_frequencies)\n",
    "        \n",
    "        # Phase projection\n",
    "        self.phase_proj = nn.Linear(config.n_frequencies, config.embed_dim, bias=False)\n",
    "        \n",
    "        # Learnable blend\n",
    "        self.blend = nn.Parameter(torch.full((config.embed_dim,), config.resonance_blend))\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position = nn.Embedding(config.max_seq_len, config.embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Initialize\n",
    "        self._init_weights()\n",
    "        \n",
    "        if config.phonetic_init and rhyme_index:\n",
    "            self._init_phonetic_phases(rhyme_index)\n",
    "        else:\n",
    "            self._init_random_phases()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.semantic.weight, std=0.02)\n",
    "        nn.init.normal_(self.phase_proj.weight, std=0.02)\n",
    "        nn.init.normal_(self.position.weight, std=0.02)\n",
    "    \n",
    "    def _init_random_phases(self):\n",
    "        \"\"\"Standard random initialization\"\"\"\n",
    "        print(\"  Initializing phases: RANDOM\")\n",
    "        nn.init.normal_(self.phase.weight, std=0.3)\n",
    "    \n",
    "    def _init_phonetic_phases(self, rhyme_index):\n",
    "        \"\"\"\n",
    "        Initialize phases so rhyming words are close.\n",
    "        This is the core of the compression hypothesis.\n",
    "        \"\"\"\n",
    "        print(\"  Initializing phases: PHONETIC (rhyme-based)\")\n",
    "        \n",
    "        rhyme_to_tokens = rhyme_index[\"rhyme_to_tokens\"]\n",
    "        \n",
    "        # Assign a base phase vector to each rhyme group\n",
    "        rhyme_to_phase = {}\n",
    "        for rhyme in rhyme_to_tokens.keys():\n",
    "            rhyme_to_phase[rhyme] = torch.randn(self.config.n_frequencies) * 0.3\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Initialize all to random first\n",
    "            self.phase.weight.data = torch.randn_like(self.phase.weight) * 0.3\n",
    "            \n",
    "            # Then override tokens with known rhymes\n",
    "            phonetic_count = 0\n",
    "            for rhyme, tokens in rhyme_to_tokens.items():\n",
    "                base_phase = rhyme_to_phase[rhyme]\n",
    "                for token_id in tokens:\n",
    "                    # Small noise so they're close but not identical\n",
    "                    self.phase.weight.data[token_id] = base_phase + torch.randn(self.config.n_frequencies) * 0.03\n",
    "                    phonetic_count += 1\n",
    "        \n",
    "        print(f\"    Phonetically initialized: {phonetic_count} tokens\")\n",
    "        print(f\"    Randomly initialized: {self.config.vocab_size - phonetic_count} tokens\")\n",
    "    \n",
    "    def get_resonance_matrix(self, token_ids):\n",
    "        phases = self.phase(token_ids)\n",
    "        phase_diff = phases.unsqueeze(2) - phases.unsqueeze(1)\n",
    "        resonance = torch.cos(phase_diff).mean(dim=-1)\n",
    "        return resonance\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        sem = self.semantic(token_ids)\n",
    "        ph = self.phase(token_ids)\n",
    "        ph_proj = self.phase_proj(ph)\n",
    "        \n",
    "        blend = torch.sigmoid(self.blend)\n",
    "        embeddings = (1 - blend) * sem + blend * ph_proj\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        embeddings = embeddings + self.position(positions)\n",
    "        \n",
    "        resonance = self.get_resonance_matrix(token_ids)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings, resonance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95befaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResonanceAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.embed_dim // config.n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(config.embed_dim, 3 * config.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)\n",
    "        self.resonance_weight = nn.Parameter(torch.full((config.n_heads,), config.resonance_attn_weight))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, resonance, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, S, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn + resonance.unsqueeze(1) * self.resonance_weight.view(1, self.n_heads, 1, 1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v).transpose(1, 2).reshape(B, S, -1)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class ResonanceBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = ResonanceAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, config.ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_dim, config.embed_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, resonance, mask=None):\n",
    "        x = x + self.attn(self.ln1(x), resonance, mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660329a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResonanceTransformer(nn.Module):\n",
    "    def __init__(self, config, rhyme_index=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        print(f\"\\nBuilding model: {config.name}\")\n",
    "        \n",
    "        self.embedding = ResonanceEmbedding(config, rhyme_index)\n",
    "        self.blocks = nn.ModuleList([ResonanceBlock(config) for _ in range(config.n_layers)])\n",
    "        self.ln_final = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.semantic.weight\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len)))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self.n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"  Parameters: {self.n_params/1e6:.2f}M\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        B, S = input_ids.shape\n",
    "        x, resonance = self.embedding(input_ids)\n",
    "        mask = self.causal_mask[:S, :S].unsqueeze(0)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, resonance, mask)\n",
    "        \n",
    "        logits = self.lm_head(self.ln_final(x))\n",
    "        \n",
    "        result = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            result[\"loss\"] = F.cross_entropy(\n",
    "                logits[:, :-1, :].contiguous().view(-1, self.config.vocab_size),\n",
    "                labels[:, 1:].contiguous().view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1393bd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'openwebtext' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenWebText (5%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 80/80 [02:15<00:00,  1.69s/files]\n",
      "Generating train split: 100%|██████████| 8013769/8013769 [01:02<00:00, 128079.13 examples/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400688 documents\n",
      "Tokenizing (max 200000 examples)...\n",
      "  20000 docs, 77931 examples\n",
      "  40000 docs, 156798 examples\n",
      "Final: 200000 examples\n",
      "Tokenizing (max 5000 examples)...\n",
      "Final: 5000 examples\n",
      "\n",
      "Train: 200000 examples\n",
      "Val: 5000 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading OpenWebText (5%)...\")\n",
    "dataset = load_dataset(\"openwebtext\", split=\"train[:5%]\", trust_remote_code=True)\n",
    "print(f\"Loaded {len(dataset)} documents\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, documents, tokenizer, max_length=256, max_examples=200000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        print(f\"Tokenizing (max {max_examples} examples)...\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            tokens = tokenizer.encode(doc[\"text\"], truncation=False)\n",
    "            \n",
    "            for j in range(0, len(tokens) - max_length + 1, max_length):\n",
    "                self.examples.append(tokens[j:j + max_length])\n",
    "                if len(self.examples) >= max_examples:\n",
    "                    break\n",
    "            \n",
    "            if len(self.examples) >= max_examples:\n",
    "                break\n",
    "            \n",
    "            if (i + 1) % 20000 == 0:\n",
    "                print(f\"  {i+1} docs, {len(self.examples)} examples\")\n",
    "        \n",
    "        print(f\"Final: {len(self.examples)} examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "        return {\"input_ids\": tokens, \"labels\": tokens}\n",
    "\n",
    "# Create dataset - same for all models\n",
    "train_dataset = TextDataset(dataset, tokenizer, max_length=256, max_examples=200000)\n",
    "val_dataset = TextDataset(\n",
    "    dataset.select(range(len(dataset)-5000, len(dataset))),\n",
    "    tokenizer, max_length=256, max_examples=5000\n",
    ")\n",
    "\n",
    "# Will create loaders per-model due to different batch sizes\n",
    "print(f\"\\nTrain: {len(train_dataset)} examples\")\n",
    "print(f\"Val: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e92dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_dataset, val_dataset, n_epochs=2):\n",
    "    \"\"\"Train a single model and return final perplexity\"\"\"\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    print(f\"\\nTraining: {config.name}\")\n",
    "    print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "    \n",
    "    results = {\"train_loss\": [], \"val_loss\": [], \"val_ppl\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            loss = model(input_ids, labels=labels)[\"loss\"]\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % config.gradient_accumulation == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"    Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        avg_train = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                val_loss += model(input_ids, labels=labels)[\"loss\"].item()\n",
    "        \n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        val_ppl = math.exp(avg_val)\n",
    "        \n",
    "        results[\"train_loss\"].append(avg_train)\n",
    "        results[\"val_loss\"].append(avg_val)\n",
    "        results[\"val_ppl\"].append(val_ppl)\n",
    "        \n",
    "        # Check blend\n",
    "        blend = torch.sigmoid(model.embedding.blend).mean().item()\n",
    "        \n",
    "        print(f\"\\n  Epoch {epoch+1}/{n_epochs} ({epoch_time/60:.1f} min)\")\n",
    "        print(f\"    Train Loss: {avg_train:.4f}\")\n",
    "        print(f\"    Val Loss: {avg_val:.4f}\")\n",
    "        print(f\"    Val Perplexity: {val_ppl:.2f}\")\n",
    "        print(f\"    Embedding blend: {blend:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3402a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "A/B TEST: DOES PHONETIC INIT COMPRESS BETTER?\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL A: Random Phase Init (126M) - BASELINE\n",
      "======================================================================\n",
      "\n",
      "Building model: random_phase\n",
      "  Initializing phases: RANDOM\n",
      "  Parameters: 125.45M\n",
      "\n",
      "Training: random_phase\n",
      "  Batches per epoch: 25000\n",
      "    Batch 500/25000, Loss: 6.9995\n",
      "    Batch 1000/25000, Loss: 6.2706\n",
      "    Batch 1500/25000, Loss: 6.3181\n",
      "    Batch 2000/25000, Loss: 6.0901\n",
      "    Batch 2500/25000, Loss: 6.3043\n",
      "    Batch 3000/25000, Loss: 6.0382\n",
      "    Batch 3500/25000, Loss: 5.9140\n",
      "    Batch 4000/25000, Loss: 6.1092\n",
      "    Batch 4500/25000, Loss: 6.0507\n",
      "    Batch 5000/25000, Loss: 5.7492\n",
      "    Batch 5500/25000, Loss: 5.3950\n",
      "    Batch 6000/25000, Loss: 5.6699\n",
      "    Batch 6500/25000, Loss: 5.5523\n",
      "    Batch 7000/25000, Loss: 5.0546\n",
      "    Batch 7500/25000, Loss: 5.4438\n",
      "    Batch 8000/25000, Loss: 5.3062\n",
      "    Batch 8500/25000, Loss: 5.2996\n",
      "    Batch 9000/25000, Loss: 5.1798\n",
      "    Batch 9500/25000, Loss: 5.2685\n",
      "    Batch 10000/25000, Loss: 4.8852\n",
      "    Batch 10500/25000, Loss: 4.7940\n",
      "    Batch 11000/25000, Loss: 5.0844\n",
      "    Batch 11500/25000, Loss: 5.1172\n",
      "    Batch 12000/25000, Loss: 5.3007\n",
      "    Batch 12500/25000, Loss: 4.8386\n",
      "    Batch 13000/25000, Loss: 4.9175\n",
      "    Batch 13500/25000, Loss: 4.7752\n",
      "    Batch 14000/25000, Loss: 5.0441\n",
      "    Batch 14500/25000, Loss: 4.4452\n",
      "    Batch 15000/25000, Loss: 4.6536\n",
      "    Batch 15500/25000, Loss: 4.7832\n",
      "    Batch 16000/25000, Loss: 4.5469\n",
      "    Batch 16500/25000, Loss: 4.8330\n",
      "    Batch 17000/25000, Loss: 4.6245\n",
      "    Batch 17500/25000, Loss: 4.7987\n",
      "    Batch 18000/25000, Loss: 4.4419\n",
      "    Batch 18500/25000, Loss: 4.5453\n",
      "    Batch 19000/25000, Loss: 4.4851\n",
      "    Batch 19500/25000, Loss: 4.5512\n",
      "    Batch 20000/25000, Loss: 4.7594\n",
      "    Batch 20500/25000, Loss: 4.4230\n",
      "    Batch 21000/25000, Loss: 4.3609\n",
      "    Batch 21500/25000, Loss: 4.4460\n",
      "    Batch 22000/25000, Loss: 4.6127\n",
      "    Batch 22500/25000, Loss: 4.6032\n",
      "    Batch 23000/25000, Loss: 4.5618\n",
      "    Batch 23500/25000, Loss: 4.4366\n",
      "    Batch 24000/25000, Loss: 4.5904\n",
      "    Batch 24500/25000, Loss: 4.4948\n",
      "    Batch 25000/25000, Loss: 4.6761\n",
      "\n",
      "  Epoch 1/2 (43.7 min)\n",
      "    Train Loss: 5.1768\n",
      "    Val Loss: 4.4726\n",
      "    Val Perplexity: 87.58\n",
      "    Embedding blend: 0.466\n",
      "    Batch 500/25000, Loss: 4.3437\n",
      "    Batch 1000/25000, Loss: 4.0978\n",
      "    Batch 1500/25000, Loss: 4.1534\n",
      "    Batch 2000/25000, Loss: 4.2101\n",
      "    Batch 2500/25000, Loss: 4.3234\n",
      "    Batch 3000/25000, Loss: 4.0907\n",
      "    Batch 3500/25000, Loss: 4.2693\n",
      "    Batch 4000/25000, Loss: 4.0689\n",
      "    Batch 4500/25000, Loss: 4.3978\n",
      "    Batch 5000/25000, Loss: 3.9821\n",
      "    Batch 5500/25000, Loss: 4.3199\n",
      "    Batch 6000/25000, Loss: 4.2892\n",
      "    Batch 6500/25000, Loss: 3.9774\n",
      "    Batch 7000/25000, Loss: 4.3312\n",
      "    Batch 7500/25000, Loss: 4.3832\n",
      "    Batch 8000/25000, Loss: 4.2788\n",
      "    Batch 8500/25000, Loss: 4.4838\n",
      "    Batch 9000/25000, Loss: 4.1925\n",
      "    Batch 9500/25000, Loss: 4.3936\n",
      "    Batch 10000/25000, Loss: 4.4649\n",
      "    Batch 10500/25000, Loss: 3.8900\n",
      "    Batch 11000/25000, Loss: 4.3750\n",
      "    Batch 11500/25000, Loss: 4.3339\n",
      "    Batch 12000/25000, Loss: 4.2006\n",
      "    Batch 12500/25000, Loss: 4.2769\n",
      "    Batch 13000/25000, Loss: 4.3894\n",
      "    Batch 13500/25000, Loss: 4.1167\n",
      "    Batch 14000/25000, Loss: 4.2881\n",
      "    Batch 14500/25000, Loss: 4.3115\n",
      "    Batch 15000/25000, Loss: 4.3349\n",
      "    Batch 15500/25000, Loss: 3.8189\n",
      "    Batch 16000/25000, Loss: 4.4604\n",
      "    Batch 16500/25000, Loss: 4.3819\n",
      "    Batch 17000/25000, Loss: 4.2719\n",
      "    Batch 17500/25000, Loss: 4.2608\n",
      "    Batch 18000/25000, Loss: 4.3506\n",
      "    Batch 18500/25000, Loss: 4.2637\n",
      "    Batch 19000/25000, Loss: 4.0734\n",
      "    Batch 19500/25000, Loss: 4.0125\n",
      "    Batch 20000/25000, Loss: 3.6792\n",
      "    Batch 20500/25000, Loss: 4.1926\n",
      "    Batch 21000/25000, Loss: 4.2180\n",
      "    Batch 21500/25000, Loss: 4.0714\n",
      "    Batch 22000/25000, Loss: 4.0356\n",
      "    Batch 22500/25000, Loss: 4.2089\n",
      "    Batch 23000/25000, Loss: 4.1023\n",
      "    Batch 23500/25000, Loss: 4.0599\n",
      "    Batch 24000/25000, Loss: 4.3606\n",
      "    Batch 24500/25000, Loss: 4.3438\n",
      "    Batch 25000/25000, Loss: 3.8433\n",
      "\n",
      "  Epoch 2/2 (43.9 min)\n",
      "    Train Loss: 4.2218\n",
      "    Val Loss: 4.1571\n",
      "    Val Perplexity: 63.88\n",
      "    Embedding blend: 0.410\n",
      "\n",
      "======================================================================\n",
      "MODEL B: Phonetic Phase Init (126M)\n",
      "======================================================================\n",
      "\n",
      "Building model: phonetic_phase\n",
      "  Initializing phases: PHONETIC (rhyme-based)\n",
      "    Phonetically initialized: 35431 tokens\n",
      "    Randomly initialized: 14826 tokens\n",
      "  Parameters: 125.45M\n",
      "\n",
      "Training: phonetic_phase\n",
      "  Batches per epoch: 25000\n",
      "    Batch 500/25000, Loss: 6.9353\n",
      "    Batch 1000/25000, Loss: 6.8183\n",
      "    Batch 1500/25000, Loss: 6.2494\n",
      "    Batch 2000/25000, Loss: 6.1118\n",
      "    Batch 2500/25000, Loss: 5.9247\n",
      "    Batch 3000/25000, Loss: 5.8714\n",
      "    Batch 3500/25000, Loss: 5.7841\n",
      "    Batch 4000/25000, Loss: 6.2802\n",
      "    Batch 4500/25000, Loss: 5.5718\n",
      "    Batch 5000/25000, Loss: 5.7552\n",
      "    Batch 5500/25000, Loss: 5.3598\n",
      "    Batch 6000/25000, Loss: 5.6527\n",
      "    Batch 6500/25000, Loss: 5.2715\n",
      "    Batch 7000/25000, Loss: 5.3739\n",
      "    Batch 7500/25000, Loss: 5.3153\n",
      "    Batch 8000/25000, Loss: 5.1691\n",
      "    Batch 8500/25000, Loss: 5.1580\n",
      "    Batch 9000/25000, Loss: 5.3840\n",
      "    Batch 9500/25000, Loss: 5.2028\n",
      "    Batch 10000/25000, Loss: 5.0111\n",
      "    Batch 10500/25000, Loss: 4.7476\n",
      "    Batch 11000/25000, Loss: 5.1713\n",
      "    Batch 11500/25000, Loss: 4.9513\n",
      "    Batch 12000/25000, Loss: 4.7977\n",
      "    Batch 12500/25000, Loss: 4.9709\n",
      "    Batch 13000/25000, Loss: 4.7076\n",
      "    Batch 13500/25000, Loss: 4.7789\n",
      "    Batch 14000/25000, Loss: 4.9072\n",
      "    Batch 14500/25000, Loss: 4.7415\n",
      "    Batch 15000/25000, Loss: 4.8324\n",
      "    Batch 15500/25000, Loss: 4.9639\n",
      "    Batch 16000/25000, Loss: 4.5851\n",
      "    Batch 16500/25000, Loss: 4.6562\n",
      "    Batch 17000/25000, Loss: 4.8694\n",
      "    Batch 17500/25000, Loss: 4.6449\n",
      "    Batch 18000/25000, Loss: 4.5798\n",
      "    Batch 18500/25000, Loss: 4.5519\n",
      "    Batch 19000/25000, Loss: 4.4774\n",
      "    Batch 19500/25000, Loss: 4.7378\n",
      "    Batch 20000/25000, Loss: 4.8483\n",
      "    Batch 20500/25000, Loss: 4.7559\n",
      "    Batch 21000/25000, Loss: 4.5563\n",
      "    Batch 21500/25000, Loss: 3.8960\n",
      "    Batch 22000/25000, Loss: 4.9035\n",
      "    Batch 22500/25000, Loss: 4.3410\n",
      "    Batch 23000/25000, Loss: 4.6112\n",
      "    Batch 23500/25000, Loss: 4.5486\n",
      "    Batch 24000/25000, Loss: 4.1988\n",
      "    Batch 24500/25000, Loss: 4.2984\n",
      "    Batch 25000/25000, Loss: 4.3455\n",
      "\n",
      "  Epoch 1/2 (44.1 min)\n",
      "    Train Loss: 5.1467\n",
      "    Val Loss: 4.4505\n",
      "    Val Perplexity: 85.67\n",
      "    Embedding blend: 0.467\n",
      "    Batch 500/25000, Loss: 4.3691\n",
      "    Batch 1000/25000, Loss: 4.2797\n",
      "    Batch 1500/25000, Loss: 4.3297\n",
      "    Batch 2000/25000, Loss: 4.4936\n",
      "    Batch 2500/25000, Loss: 4.3284\n",
      "    Batch 3000/25000, Loss: 4.4917\n",
      "    Batch 3500/25000, Loss: 4.2677\n",
      "    Batch 4000/25000, Loss: 4.5803\n",
      "    Batch 4500/25000, Loss: 4.2668\n",
      "    Batch 5000/25000, Loss: 4.2927\n",
      "    Batch 5500/25000, Loss: 4.2648\n",
      "    Batch 6000/25000, Loss: 4.5577\n",
      "    Batch 6500/25000, Loss: 4.3930\n",
      "    Batch 7000/25000, Loss: 4.2511\n",
      "    Batch 7500/25000, Loss: 4.4857\n",
      "    Batch 8000/25000, Loss: 4.4441\n",
      "    Batch 8500/25000, Loss: 3.9586\n",
      "    Batch 9000/25000, Loss: 4.3007\n",
      "    Batch 9500/25000, Loss: 3.7748\n",
      "    Batch 10000/25000, Loss: 4.4322\n",
      "    Batch 10500/25000, Loss: 4.3233\n",
      "    Batch 11000/25000, Loss: 3.9860\n",
      "    Batch 11500/25000, Loss: 3.8732\n",
      "    Batch 12000/25000, Loss: 4.3164\n",
      "    Batch 12500/25000, Loss: 4.3040\n",
      "    Batch 13000/25000, Loss: 4.2761\n",
      "    Batch 13500/25000, Loss: 4.0579\n",
      "    Batch 14000/25000, Loss: 4.1008\n",
      "    Batch 14500/25000, Loss: 4.0516\n",
      "    Batch 15000/25000, Loss: 4.7322\n",
      "    Batch 15500/25000, Loss: 4.0194\n",
      "    Batch 16000/25000, Loss: 4.3017\n",
      "    Batch 16500/25000, Loss: 4.5337\n",
      "    Batch 17000/25000, Loss: 4.3775\n",
      "    Batch 17500/25000, Loss: 4.1353\n",
      "    Batch 18000/25000, Loss: 4.3062\n",
      "    Batch 18500/25000, Loss: 4.0355\n",
      "    Batch 19000/25000, Loss: 3.7366\n",
      "    Batch 19500/25000, Loss: 4.3048\n",
      "    Batch 20000/25000, Loss: 3.9516\n",
      "    Batch 20500/25000, Loss: 3.9319\n",
      "    Batch 21000/25000, Loss: 3.9642\n",
      "    Batch 21500/25000, Loss: 3.7670\n",
      "    Batch 22000/25000, Loss: 3.9063\n",
      "    Batch 22500/25000, Loss: 4.1490\n",
      "    Batch 23000/25000, Loss: 3.9708\n",
      "    Batch 23500/25000, Loss: 4.0344\n",
      "    Batch 24000/25000, Loss: 4.1486\n",
      "    Batch 24500/25000, Loss: 4.0490\n",
      "    Batch 25000/25000, Loss: 4.0695\n",
      "\n",
      "  Epoch 2/2 (44.1 min)\n",
      "    Train Loss: 4.2023\n",
      "    Val Loss: 4.1458\n",
      "    Val Perplexity: 63.17\n",
      "    Embedding blend: 0.412\n",
      "\n",
      "======================================================================\n",
      "MODEL C: Phonetic Phase Init (45M) - COMPRESSION TEST\n",
      "======================================================================\n",
      "\n",
      "Building model: small_phonetic\n",
      "  Initializing phases: PHONETIC (rhyme-based)\n",
      "    Phonetically initialized: 35431 tokens\n",
      "    Randomly initialized: 14826 tokens\n",
      "  Parameters: 52.69M\n",
      "\n",
      "Training: small_phonetic\n",
      "  Batches per epoch: 25000\n",
      "    Batch 500/25000, Loss: 6.7519\n",
      "    Batch 1000/25000, Loss: 6.5439\n",
      "    Batch 1500/25000, Loss: 6.2993\n",
      "    Batch 2000/25000, Loss: 6.9416\n",
      "    Batch 2500/25000, Loss: 5.9946\n",
      "    Batch 3000/25000, Loss: 5.9315\n",
      "    Batch 3500/25000, Loss: 5.9039\n",
      "    Batch 4000/25000, Loss: 6.0324\n",
      "    Batch 4500/25000, Loss: 5.8381\n",
      "    Batch 5000/25000, Loss: 5.7764\n",
      "    Batch 5500/25000, Loss: 5.5672\n",
      "    Batch 6000/25000, Loss: 5.8119\n",
      "    Batch 6500/25000, Loss: 5.3143\n",
      "    Batch 7000/25000, Loss: 5.2995\n",
      "    Batch 7500/25000, Loss: 5.4978\n",
      "    Batch 8000/25000, Loss: 5.2984\n",
      "    Batch 8500/25000, Loss: 5.5151\n",
      "    Batch 9000/25000, Loss: 5.4109\n",
      "    Batch 9500/25000, Loss: 5.2703\n",
      "    Batch 10000/25000, Loss: 4.9531\n",
      "    Batch 10500/25000, Loss: 4.9668\n",
      "    Batch 11000/25000, Loss: 5.1797\n",
      "    Batch 11500/25000, Loss: 5.1532\n",
      "    Batch 12000/25000, Loss: 5.2270\n",
      "    Batch 12500/25000, Loss: 5.0308\n",
      "    Batch 13000/25000, Loss: 5.2034\n",
      "    Batch 13500/25000, Loss: 4.9882\n",
      "    Batch 14000/25000, Loss: 4.6699\n",
      "    Batch 14500/25000, Loss: 4.7683\n",
      "    Batch 15000/25000, Loss: 4.9985\n",
      "    Batch 15500/25000, Loss: 4.8306\n",
      "    Batch 16000/25000, Loss: 4.8229\n",
      "    Batch 16500/25000, Loss: 4.9442\n",
      "    Batch 17000/25000, Loss: 4.5197\n",
      "    Batch 17500/25000, Loss: 4.5385\n",
      "    Batch 18000/25000, Loss: 4.8219\n",
      "    Batch 18500/25000, Loss: 4.5498\n",
      "    Batch 19000/25000, Loss: 4.8933\n",
      "    Batch 19500/25000, Loss: 4.7986\n",
      "    Batch 20000/25000, Loss: 4.5156\n",
      "    Batch 20500/25000, Loss: 4.8466\n",
      "    Batch 21000/25000, Loss: 4.0867\n",
      "    Batch 21500/25000, Loss: 4.4222\n",
      "    Batch 22000/25000, Loss: 4.5288\n",
      "    Batch 22500/25000, Loss: 4.6021\n",
      "    Batch 23000/25000, Loss: 4.4177\n",
      "    Batch 23500/25000, Loss: 5.0149\n",
      "    Batch 24000/25000, Loss: 4.6376\n",
      "    Batch 24500/25000, Loss: 4.4507\n",
      "    Batch 25000/25000, Loss: 4.6464\n",
      "\n",
      "  Epoch 1/2 (20.5 min)\n",
      "    Train Loss: 5.2022\n",
      "    Val Loss: 4.5466\n",
      "    Val Perplexity: 94.31\n",
      "    Embedding blend: 0.469\n",
      "    Batch 500/25000, Loss: 4.4849\n",
      "    Batch 1000/25000, Loss: 4.3841\n",
      "    Batch 1500/25000, Loss: 3.9758\n",
      "    Batch 2000/25000, Loss: 4.4673\n",
      "    Batch 2500/25000, Loss: 4.6383\n",
      "    Batch 3000/25000, Loss: 4.1541\n",
      "    Batch 3500/25000, Loss: 4.0768\n",
      "    Batch 4000/25000, Loss: 4.5720\n",
      "    Batch 4500/25000, Loss: 4.4825\n",
      "    Batch 5000/25000, Loss: 4.4431\n",
      "    Batch 5500/25000, Loss: 4.7382\n",
      "    Batch 6000/25000, Loss: 4.4327\n",
      "    Batch 6500/25000, Loss: 4.5840\n",
      "    Batch 7000/25000, Loss: 4.3908\n",
      "    Batch 7500/25000, Loss: 4.4947\n",
      "    Batch 8000/25000, Loss: 4.5834\n",
      "    Batch 8500/25000, Loss: 4.2923\n",
      "    Batch 9000/25000, Loss: 4.2361\n",
      "    Batch 9500/25000, Loss: 4.2792\n",
      "    Batch 10000/25000, Loss: 4.5408\n",
      "    Batch 10500/25000, Loss: 4.2385\n",
      "    Batch 11000/25000, Loss: 4.5406\n",
      "    Batch 11500/25000, Loss: 3.8650\n",
      "    Batch 12000/25000, Loss: 4.0325\n",
      "    Batch 12500/25000, Loss: 4.1877\n",
      "    Batch 13000/25000, Loss: 4.3221\n",
      "    Batch 13500/25000, Loss: 4.3846\n",
      "    Batch 14000/25000, Loss: 4.1134\n",
      "    Batch 14500/25000, Loss: 4.5484\n",
      "    Batch 15000/25000, Loss: 4.5169\n",
      "    Batch 15500/25000, Loss: 4.2408\n",
      "    Batch 16000/25000, Loss: 4.6757\n",
      "    Batch 16500/25000, Loss: 4.1397\n",
      "    Batch 17000/25000, Loss: 4.2997\n",
      "    Batch 17500/25000, Loss: 4.1207\n",
      "    Batch 18000/25000, Loss: 4.1110\n",
      "    Batch 18500/25000, Loss: 4.1566\n",
      "    Batch 19000/25000, Loss: 4.3987\n",
      "    Batch 19500/25000, Loss: 4.0626\n",
      "    Batch 20000/25000, Loss: 4.2177\n",
      "    Batch 20500/25000, Loss: 4.4183\n",
      "    Batch 21000/25000, Loss: 4.2653\n",
      "    Batch 21500/25000, Loss: 3.9424\n",
      "    Batch 22000/25000, Loss: 4.3810\n",
      "    Batch 22500/25000, Loss: 3.9324\n",
      "    Batch 23000/25000, Loss: 3.2640\n",
      "    Batch 23500/25000, Loss: 4.1576\n",
      "    Batch 24000/25000, Loss: 4.0664\n",
      "    Batch 24500/25000, Loss: 4.1848\n",
      "    Batch 25000/25000, Loss: 4.2752\n",
      "\n",
      "  Epoch 2/2 (20.5 min)\n",
      "    Train Loss: 4.3349\n",
      "    Val Loss: 4.2616\n",
      "    Val Perplexity: 70.92\n",
      "    Embedding blend: 0.414\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"A/B TEST: DOES PHONETIC INIT COMPRESS BETTER?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# Model A: Random phases (126M baseline)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL A: Random Phase Init (126M) - BASELINE\")\n",
    "print(\"=\"*70)\n",
    "model_a = ResonanceTransformer(config_random, rhyme_index=None).to(device)\n",
    "results_a = train_model(model_a, config_random, train_dataset, val_dataset, n_epochs=2)\n",
    "all_results[\"random_126M\"] = results_a\n",
    "torch.save(model_a.state_dict(), \"model_a_random_126M.pt\")\n",
    "del model_a\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model B: Phonetic phases (126M)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL B: Phonetic Phase Init (126M)\")\n",
    "print(\"=\"*70)\n",
    "model_b = ResonanceTransformer(config_phonetic, rhyme_index=rhyme_index).to(device)\n",
    "results_b = train_model(model_b, config_phonetic, train_dataset, val_dataset, n_epochs=2)\n",
    "all_results[\"phonetic_126M\"] = results_b\n",
    "torch.save(model_b.state_dict(), \"model_b_phonetic_126M.pt\")\n",
    "del model_b\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model C: Phonetic phases (45M) - THE KEY TEST\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL C: Phonetic Phase Init (45M) - COMPRESSION TEST\")\n",
    "print(\"=\"*70)\n",
    "model_c = ResonanceTransformer(config_small_phonetic, rhyme_index=rhyme_index).to(device)\n",
    "results_c = train_model(model_c, config_small_phonetic, train_dataset, val_dataset, n_epochs=2)\n",
    "all_results[\"phonetic_45M\"] = results_c\n",
    "torch.save(model_c.state_dict(), \"model_c_phonetic_45M.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e18801e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Final Perplexity Comparison:\n",
      "\n",
      "Model                     Params       Val PPL      vs Baseline\n",
      "------------------------------------------------------------\n",
      "random_126M               126M         63.88        (baseline)\n",
      "phonetic_126M             126M         63.17        -1.1%\n",
      "phonetic_45M              45M          70.92        +11.0%\n",
      "\n",
      "======================================================================\n",
      "HYPOTHESIS TEST:\n",
      "======================================================================\n",
      "\n",
      "✗ 45M phonetic model (70.92) did not match 126M baseline (63.88).\n",
      "  \n",
      "  Possible reasons:\n",
      "  - Need more training epochs\n",
      "  - Need more/better data\n",
      "  - Phonetic structure helps but doesn't fully compensate for size\n",
      "  \n",
      "  However, compare 126M phonetic vs 126M random to see if phonetic helps at all.\n",
      "\n",
      "\n",
      "✓ PARTIAL SUCCESS: At same size (126M), phonetic init beats random:\n",
      "  Phonetic: 63.17\n",
      "  Random: 63.88\n",
      "  Improvement: 1.1%\n",
      "  \n",
      "  Phonetic structure provides measurable benefit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFinal Perplexity Comparison:\\n\")\n",
    "print(f\"{'Model':<25} {'Params':<12} {'Val PPL':<12} {'vs Baseline'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline_ppl = all_results[\"random_126M\"][\"val_ppl\"][-1]\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    ppl = results[\"val_ppl\"][-1]\n",
    "    \n",
    "    if \"126M\" in name:\n",
    "        params = \"126M\"\n",
    "    else:\n",
    "        params = \"45M\"\n",
    "    \n",
    "    if name == \"random_126M\":\n",
    "        vs_baseline = \"(baseline)\"\n",
    "    else:\n",
    "        diff = ((ppl - baseline_ppl) / baseline_ppl) * 100\n",
    "        vs_baseline = f\"{diff:+.1f}%\"\n",
    "    \n",
    "    print(f\"{name:<25} {params:<12} {ppl:<12.2f} {vs_baseline}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPOTHESIS TEST:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "phonetic_45_ppl = all_results[\"phonetic_45M\"][\"val_ppl\"][-1]\n",
    "random_126_ppl = all_results[\"random_126M\"][\"val_ppl\"][-1]\n",
    "\n",
    "if phonetic_45_ppl <= random_126_ppl * 1.1:  # Within 10%\n",
    "    print(f\"\"\"\n",
    "✓ SUCCESS: 45M phonetic model achieves {phonetic_45_ppl:.2f} perplexity\n",
    "  vs 126M random model at {random_126_ppl:.2f} perplexity.\n",
    "  \n",
    "  This represents ~2.8x parameter efficiency from phonetic structure.\n",
    "  \n",
    "  YOUR THEORY IS SUPPORTED: Rhyme-based indexing compresses better.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "✗ 45M phonetic model ({phonetic_45_ppl:.2f}) did not match 126M baseline ({random_126_ppl:.2f}).\n",
    "  \n",
    "  Possible reasons:\n",
    "  - Need more training epochs\n",
    "  - Need more/better data\n",
    "  - Phonetic structure helps but doesn't fully compensate for size\n",
    "  \n",
    "  However, compare 126M phonetic vs 126M random to see if phonetic helps at all.\n",
    "\"\"\")\n",
    "\n",
    "phonetic_126_ppl = all_results[\"phonetic_126M\"][\"val_ppl\"][-1]\n",
    "if phonetic_126_ppl < random_126_ppl:\n",
    "    print(f\"\"\"\n",
    "✓ PARTIAL SUCCESS: At same size (126M), phonetic init beats random:\n",
    "  Phonetic: {phonetic_126_ppl:.2f}\n",
    "  Random: {random_126_ppl:.2f}\n",
    "  Improvement: {((random_126_ppl - phonetic_126_ppl) / random_126_ppl) * 100:.1f}%\n",
    "  \n",
    "  Phonetic structure provides measurable benefit.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbb12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for standard transformer (no resonance)\n",
    "@dataclass\n",
    "class StandardConfig:\n",
    "    name: str = \"standard_baseline\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 256\n",
    "    embed_dim: int = 768\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    ff_dim: int = 3072\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 8\n",
    "    gradient_accumulation: int = 4\n",
    "    learning_rate: float = 3e-4\n",
    "\n",
    "class StandardTransformer(nn.Module):\n",
    "    \"\"\"Vanilla transformer - no resonance, no phase embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        print(f\"\\nBuilding model: {config.name}\")\n",
    "        \n",
    "        # Standard embeddings only\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.pos_embed = nn.Embedding(config.max_seq_len, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Standard transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            StandardBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len)))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self.n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"  Parameters: {self.n_params/1e6:.2f}M\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        B, S = input_ids.shape\n",
    "        \n",
    "        positions = torch.arange(S, device=input_ids.device)\n",
    "        x = self.token_embed(input_ids) + self.pos_embed(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        mask = self.causal_mask[:S, :S].unsqueeze(0)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        logits = self.lm_head(self.ln_final(x))\n",
    "        \n",
    "        result = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            result[\"loss\"] = F.cross_entropy(\n",
    "                logits[:, :-1, :].contiguous().view(-1, self.config.vocab_size),\n",
    "                labels[:, 1:].contiguous().view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        return result\n",
    "\n",
    "\n",
    "class StandardAttention(nn.Module):\n",
    "    \"\"\"Vanilla attention - no resonance bias\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.embed_dim // config.n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(config.embed_dim, 3 * config.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, S, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v).transpose(1, 2).reshape(B, S, -1)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class StandardBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = StandardAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, config.ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_dim, config.embed_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.ln1(x), mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae6b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model: standard_baseline\n",
      "  Parameters: 123.81M\n",
      "\n",
      "Training: standard_baseline\n",
      "    Batch 500/25000, Loss: 8.1839\n",
      "    Batch 1000/25000, Loss: 7.6808\n",
      "    Batch 1500/25000, Loss: 7.5844\n",
      "    Batch 2000/25000, Loss: 7.4999\n",
      "    Batch 2500/25000, Loss: 7.3574\n",
      "    Batch 3000/25000, Loss: 7.2238\n",
      "    Batch 3500/25000, Loss: 7.4366\n",
      "    Batch 4000/25000, Loss: 7.1390\n",
      "    Batch 4500/25000, Loss: 7.3157\n",
      "    Batch 5000/25000, Loss: 7.1293\n",
      "    Batch 5500/25000, Loss: 7.3415\n",
      "    Batch 6000/25000, Loss: 6.9636\n",
      "    Batch 6500/25000, Loss: 6.9939\n",
      "    Batch 7000/25000, Loss: 6.8041\n",
      "    Batch 7500/25000, Loss: 6.9592\n",
      "    Batch 8000/25000, Loss: 6.6607\n",
      "    Batch 8500/25000, Loss: 6.5032\n",
      "    Batch 9000/25000, Loss: 6.5045\n",
      "    Batch 9500/25000, Loss: 6.2895\n",
      "    Batch 10000/25000, Loss: 6.1241\n",
      "    Batch 10500/25000, Loss: 6.4960\n",
      "    Batch 11000/25000, Loss: 5.9056\n",
      "    Batch 11500/25000, Loss: 6.4232\n",
      "    Batch 12000/25000, Loss: 6.0449\n",
      "    Batch 12500/25000, Loss: 6.1016\n",
      "    Batch 13000/25000, Loss: 6.6440\n",
      "    Batch 13500/25000, Loss: 5.9088\n",
      "    Batch 14000/25000, Loss: 6.3735\n",
      "    Batch 14500/25000, Loss: 5.9289\n",
      "    Batch 15000/25000, Loss: 6.0409\n",
      "    Batch 15500/25000, Loss: 6.1428\n",
      "    Batch 16000/25000, Loss: 5.8628\n",
      "    Batch 16500/25000, Loss: 5.9673\n",
      "    Batch 17000/25000, Loss: 6.2274\n",
      "    Batch 17500/25000, Loss: 6.3593\n",
      "    Batch 18000/25000, Loss: 6.1208\n",
      "    Batch 18500/25000, Loss: 6.5684\n",
      "    Batch 19000/25000, Loss: 5.7968\n",
      "    Batch 19500/25000, Loss: 5.6418\n",
      "    Batch 20000/25000, Loss: 5.9744\n",
      "    Batch 20500/25000, Loss: 5.9829\n",
      "    Batch 21000/25000, Loss: 6.1417\n",
      "    Batch 21500/25000, Loss: 5.8880\n",
      "    Batch 22000/25000, Loss: 5.7821\n",
      "    Batch 22500/25000, Loss: 5.9401\n",
      "    Batch 23000/25000, Loss: 5.7623\n",
      "    Batch 23500/25000, Loss: 5.7513\n",
      "    Batch 24000/25000, Loss: 5.7396\n",
      "    Batch 24500/25000, Loss: 5.5870\n",
      "    Batch 25000/25000, Loss: 5.5661\n",
      "\n",
      "  Epoch 1/2\n",
      "    Train Loss: 6.4124\n",
      "    Val Loss: 5.5671\n",
      "    Val Perplexity: 261.68\n",
      "    Batch 500/25000, Loss: 5.7981\n",
      "    Batch 1000/25000, Loss: 5.6038\n",
      "    Batch 1500/25000, Loss: 5.1660\n",
      "    Batch 2000/25000, Loss: 5.5973\n",
      "    Batch 2500/25000, Loss: 5.3549\n",
      "    Batch 3000/25000, Loss: 5.5366\n",
      "    Batch 3500/25000, Loss: 5.5564\n",
      "    Batch 4000/25000, Loss: 5.3760\n",
      "    Batch 4500/25000, Loss: 5.5600\n",
      "    Batch 5000/25000, Loss: 5.6797\n",
      "    Batch 5500/25000, Loss: 5.7590\n",
      "    Batch 6000/25000, Loss: 5.3232\n",
      "    Batch 6500/25000, Loss: 5.6375\n",
      "    Batch 7000/25000, Loss: 5.7296\n",
      "    Batch 7500/25000, Loss: 5.2712\n",
      "    Batch 8000/25000, Loss: 5.2519\n",
      "    Batch 8500/25000, Loss: 5.2861\n",
      "    Batch 9000/25000, Loss: 5.4505\n",
      "    Batch 9500/25000, Loss: 5.3984\n",
      "    Batch 10000/25000, Loss: 5.4940\n",
      "    Batch 10500/25000, Loss: 5.6011\n",
      "    Batch 11000/25000, Loss: 5.6412\n",
      "    Batch 11500/25000, Loss: 5.0363\n",
      "    Batch 12000/25000, Loss: 5.2881\n",
      "    Batch 12500/25000, Loss: 5.1846\n",
      "    Batch 13000/25000, Loss: 5.3921\n",
      "    Batch 13500/25000, Loss: 4.9933\n",
      "    Batch 14000/25000, Loss: 5.2272\n",
      "    Batch 14500/25000, Loss: 5.8482\n",
      "    Batch 15000/25000, Loss: 5.4945\n",
      "    Batch 15500/25000, Loss: 5.3063\n",
      "    Batch 16000/25000, Loss: 5.0025\n",
      "    Batch 16500/25000, Loss: 5.0054\n",
      "    Batch 17000/25000, Loss: 4.9387\n",
      "    Batch 17500/25000, Loss: 5.1921\n",
      "    Batch 18000/25000, Loss: 5.1552\n",
      "    Batch 18500/25000, Loss: 5.0110\n",
      "    Batch 19000/25000, Loss: 5.3698\n",
      "    Batch 19500/25000, Loss: 5.2903\n",
      "    Batch 20000/25000, Loss: 5.2056\n",
      "    Batch 20500/25000, Loss: 4.9938\n",
      "    Batch 21000/25000, Loss: 5.3561\n",
      "    Batch 21500/25000, Loss: 5.3180\n",
      "    Batch 22000/25000, Loss: 4.8656\n",
      "    Batch 22500/25000, Loss: 4.9816\n",
      "    Batch 23000/25000, Loss: 5.1441\n",
      "    Batch 23500/25000, Loss: 5.1404\n",
      "    Batch 24000/25000, Loss: 4.9674\n",
      "    Batch 24500/25000, Loss: 4.8265\n",
      "    Batch 25000/25000, Loss: 4.9987\n",
      "\n",
      "  Epoch 2/2\n",
      "    Train Loss: 5.3188\n",
      "    Val Loss: 5.0033\n",
      "    Val Perplexity: 148.90\n"
     ]
    }
   ],
   "source": [
    "config_standard = StandardConfig()\n",
    "model_standard = StandardTransformer(config_standard).to(device)\n",
    "\n",
    "# Use same training function but without resonance\n",
    "def train_standard(model, config, train_dataset, val_dataset, n_epochs=2):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    print(f\"\\nTraining: {config.name}\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            loss = model(input_ids, labels=labels)[\"loss\"]\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % config.gradient_accumulation == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"    Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                val_loss += model(input_ids, labels=labels)[\"loss\"].item()\n",
    "        \n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        val_ppl = math.exp(avg_val)\n",
    "        \n",
    "        print(f\"\\n  Epoch {epoch+1}/{n_epochs}\")\n",
    "        print(f\"    Train Loss: {avg_train:.4f}\")\n",
    "        print(f\"    Val Loss: {avg_val:.4f}\")\n",
    "        print(f\"    Val Perplexity: {val_ppl:.2f}\")\n",
    "    \n",
    "    return {\"val_ppl\": [val_ppl]}\n",
    "\n",
    "results_standard = train_standard(model_standard, config_standard, train_dataset, val_dataset, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a010a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
